{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the blog here: https://www.abhishekmamidi.com/2020/09/building-machine-learning-pipelines-using-scikit-learn-and-optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Data Scientists often build Machine learning pipelines which involves preprocessing (imputing null values, feature transformation, creating new features), modeling, hyper parameter tuning. There are many transformations that need to be done before modeling in a particular order.  Scikit learn provides us with the Pipeline class to perform those transformations in one go.\n",
    "\n",
    "\n",
    "Pipeline serves multiple purposes here (from [documentation](https://scikit-learn.org/stable/modules/compose.html)):\n",
    "- <b>Convenience and encapsulation</b>: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "- <b>Joint parameter selection</b>: You can grid search over parameters of all estimators in the pipeline at once (hyper-parameter tuning/optimization).\n",
    "- <b>Safety</b>: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video, I will show you\n",
    "- How to build pipelines using scikit-learn?\n",
    "- How to create custom transformers?\n",
    "- Hyper-parameter tuning\n",
    "\n",
    "For the entire analysis, I am using the Titanic dataset. I chose this dataset because most of them are familiar with this dataset. Letâ€™s start the analysis by loading all the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T10:48:29.156187Z",
     "start_time": "2020-09-19T10:48:29.152176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T06:20:25.767655Z",
     "start_time": "2020-09-19T06:20:25.759129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_data(PATH):\n",
    "    data = pd.read_csv(PATH)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T06:20:32.106626Z",
     "start_time": "2020-09-19T06:20:26.282613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read titanic data\n",
    "\n",
    "titanic_data = load_data('https://raw.githubusercontent.com/mattdelhey/kaggle-titanic/master/Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T06:20:32.144732Z",
     "start_time": "2020-09-19T06:20:32.108634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      sex   age  sibsp  parch            ticket     fare cabin embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(survived      0\n",
       " pclass        0\n",
       " name          0\n",
       " sex           0\n",
       " age         177\n",
       " sibsp         0\n",
       " parch         0\n",
       " ticket        0\n",
       " fare          0\n",
       " cabin       687\n",
       " embarked      2\n",
       " dtype: int64, Index(['age', 'cabin', 'embarked'], dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null value columns\n",
    "\n",
    "null_value_count = titanic_data.isnull().sum()\n",
    "features_with_null_values = null_value_count[null_value_count != 0].index\n",
    "null_value_count, features_with_null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T06:27:40.554121Z",
     "start_time": "2020-09-19T06:27:40.544126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data 80:20\n",
    "train_data, test_data = train_test_split(titanic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train_data.drop(columns=[\"survived\"])\n",
    "y_train = train_data[\"survived\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"survived\"])\n",
    "y_test = test_data[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T06:27:47.440984Z",
     "start_time": "2020-09-19T06:27:47.398372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1</td>\n",
       "      <td>Partner, Mr. Austen</td>\n",
       "      <td>male</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113043</td>\n",
       "      <td>28.5000</td>\n",
       "      <td>C124</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>2</td>\n",
       "      <td>Berriman, Mr. William John</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28425</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>3</td>\n",
       "      <td>Tikkanen, Mr. Juho</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O 2. 3101293</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>3</td>\n",
       "      <td>Hansen, Mr. Henrik Juul</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>350025</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Miss. Ebba Iris Alfrida</td>\n",
       "      <td>female</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass                                name     sex   age  sibsp  parch  \\\n",
       "331       1                 Partner, Mr. Austen    male  45.5      0      0   \n",
       "733       2          Berriman, Mr. William John    male  23.0      0      0   \n",
       "382       3                  Tikkanen, Mr. Juho    male  32.0      0      0   \n",
       "704       3             Hansen, Mr. Henrik Juul    male  26.0      1      0   \n",
       "813       3  Andersson, Miss. Ebba Iris Alfrida  female   6.0      4      2   \n",
       "\n",
       "                ticket     fare cabin embarked  \n",
       "331             113043  28.5000  C124        S  \n",
       "733              28425  13.0000   NaN        S  \n",
       "382  STON/O 2. 3101293   7.9250   NaN        S  \n",
       "704             350025   7.8542   NaN        S  \n",
       "813             347082  31.2750   NaN        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 features (10 + target) in total. We can remove high cardinal features (['name', 'ticket', 'cabin']) for this analysis from the data. We remain with 7 features. Out of 7, there are 5 numerical features ('pclass', 'age', 'sibsp', 'parch', 'fare') and 2 categorical features ('sex', 'embarked'). The preprocessing for numerical and categorical features is different. So, letâ€™s build pipelines for numerical and categorical features separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pclass', 'age', 'sibsp', 'parch', 'fare'], dtype='object') Index(['sex', 'embarked'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "drop_columns = [\"name\", \"ticket\", \"cabin\"]\n",
    "numerical_columns = X_train.drop(columns=drop_columns).select_dtypes(exclude = \"object\").columns\n",
    "categorical_columns = X_train.drop(columns=drop_columns).select_dtypes(include = \"object\").columns\n",
    "print(numerical_columns, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Numerical features\n",
    "- Impute null values with median\n",
    "- Create new features. We can create 'family_count' by adding 'sibsp' (No. of siblings/spouses) and 'parch' (No. of parents/children)\n",
    "- Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn provides a lot of transformers by default. For custom processing purposes, we can create our own Custom Transformers for eg. creating new features. As scikit-learn relies on duck typing (you check only for the presence of a given method or attribute), itâ€™s very easy to create custom transformers just by implementing fit, transform and fit_transformer methods in a class.\n",
    "\n",
    "For the fit() function, you can just return self. The main processing code will go into the transform() function. The fit_transform() is automatically available for us if we add TransformerMixin as a base class. We can also add BaseEstimator as a base class which automatically provides two functions: get_params() and set_params()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New features\n",
    "\n",
    "class CreateNewFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Create a new feature 'family_count' by adding 'sibsp' and 'parch'\n",
    "        \"\"\"\n",
    "        sibsp = self.indices[0]\n",
    "        parch = self.indices[1]\n",
    "\n",
    "        family_count = X[:, sibsp] + X[:, parch]\n",
    "        X = np.c_[X, family_count]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Letâ€™s build a pipeline for processing numerical features. Itâ€™s very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [2, 3]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_params\n",
    "CreateNewFeatures([2, 3]).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>31.2750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass   age  sibsp  parch     fare\n",
       "331       1  45.5      0      0  28.5000\n",
       "733       2  23.0      0      0  13.0000\n",
       "382       3  32.0      0      0   7.9250\n",
       "704       3  26.0      1      0   7.8542\n",
       "813       3   6.0      4      2  31.2750"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[numerical_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for preprocessing numerical features\n",
    "family_count_indices = [2, 3]\n",
    "\n",
    "# List of tuples (name, transform) in a particular order. We have to provide name and transformer in each tuple\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('numerical_imputer', SimpleImputer(strategy='median')),\n",
    "    ('create_new_features', CreateNewFeatures(family_count_indices)),\n",
    "    ('feature_scaling', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('numerical_imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('create_new_features', CreateNewFeatures(indices=[2, 3])),\n",
       "                ('feature_scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Categorical features\n",
    "- Impute null values with mode\n",
    "- One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline([\n",
    "    ('categorical_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('categorical_encoder', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('categorical_imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='most_frequent',\n",
       "                               verbose=0)),\n",
       "                ('categorical_encoder',\n",
       "                 OneHotEncoder(categories='auto', drop=None,\n",
       "                               dtype=<class 'numpy.float64'>,\n",
       "                               handle_unknown='error', sparse=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformer\n",
    "\n",
    "We have applied transformations separately for numerical and categorical features. Scikit learn provides ColumnTransformer through which we can apply different transformations on different columns at the same time. Letâ€™s see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer\n",
    "column_pipeline = ColumnTransformer([\n",
    "    (\"numerical_pipeline\", numerical_pipeline, numerical_columns),\n",
    "    (\"categorical_pipeline\", categorical_pipeline, categorical_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('numerical_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('numerical_imputer',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='median',\n",
       "                                                                verbose=0)),\n",
       "                                                 ('create_new_features',\n",
       "                                                  CreateNewFeatures(indices=[2,\n",
       "                                                                             3])),\n",
       "                                                 ('feat...\n",
       "                                          steps=[('categorical_imputer',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='most_frequent',\n",
       "                                                                verbose=0)),\n",
       "                                                 ('categorical_encoder',\n",
       "                                                  OneHotEncoder(categories='auto',\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='error',\n",
       "                                                                sparse=True))],\n",
       "                                          verbose=False),\n",
       "                                 Index(['sex', 'embarked'], dtype='object'))],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each tuple takes 3 inputs: (name, pipeline, columns on which the transformation to be applied). Using ColumnTransformer, we can apply numerical and categorical transformations parallelly on the same dataset. Now, letâ€™s create the full pipeline by dropping unnecessary features before transforming the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We didn't include dropping features in the pipeline. Let's create full pipeline by dropping the features as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Drop features\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_columns):\n",
    "        self.drop_columns = drop_columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Drop features\n",
    "        \"\"\"\n",
    "        X = X.drop(columns=self.drop_columns, axis=1)\n",
    "        return X\n",
    "    \n",
    "drop_columns = [\"name\", \"ticket\", \"cabin\"]\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('drop_features', DropFeatures(drop_columns)),\n",
    "    ('column_transformer', column_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('drop_features',\n",
      "                 DropFeatures(drop_columns=['name', 'ticket', 'cabin'])),\n",
      "                ('column_transformer',\n",
      "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
      "                                   sparse_threshold=0.3,\n",
      "                                   transformer_weights=None,\n",
      "                                   transformers=[('numerical_pipeline',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=[('numerical_imputer',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_va...\n",
      "                                                           steps=[('categorical_imputer',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_value=None,\n",
      "                                                                                 missing_values=nan,\n",
      "                                                                                 strategy='most_frequent',\n",
      "                                                                                 verbose=0)),\n",
      "                                                                  ('categorical_encoder',\n",
      "                                                                   OneHotEncoder(categories='auto',\n",
      "                                                                                 drop=None,\n",
      "                                                                                 dtype=<class 'numpy.float64'>,\n",
      "                                                                                 handle_unknown='error',\n",
      "                                                                                 sparse=True))],\n",
      "                                                           verbose=False),\n",
      "                                                  Index(['sex', 'embarked'], dtype='object'))],\n",
      "                                   verbose=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We created the pipeline for processing the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of using pipelines\n",
    "- Use pipeline for preprocessing only\n",
    "- Include modeling in the pipeline\n",
    "- Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline for preprocessing features only\n",
    "We use the pipeline to pre-process the features and then do modeling on top of the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 10), (712, 11))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform input data\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "X_train.shape, X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data using XGBoost\n",
    "model = xgboost.XGBClassifier(max_depth=4)\n",
    "model.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same pipeline to transform the test data and predict using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8156424581005587, 0.762589928057554)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform test data using full_pipeline\n",
    "X_test_processed = full_pipeline.transform(X_test)\n",
    "print(X_test_processed.shape)\n",
    "\n",
    "# Predict on the processed data\n",
    "y_pred = model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred) # (0.815, 0.762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include modeling in the pipeline\n",
    "In this case, we include modeling (for eg.: DecisionTreeClassifier()) in the pipeline by adding it to full_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DecisionTreeClassifier to the end of processing pipeline\n",
    "pipeline_modeling = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipeline object, we can directly fit and predict on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('drop_features',\n",
       "                                  DropFeatures(drop_columns=['name', 'ticket',\n",
       "                                                             'cabin'])),\n",
       "                                 ('column_transformer',\n",
       "                                  ColumnTransformer(n_jobs=None,\n",
       "                                                    remainder='drop',\n",
       "                                                    sparse_threshold=0.3,\n",
       "                                                    transformer_weights=None,\n",
       "                                                    transformers=[('numerical_pipeline',\n",
       "                                                                   Pipeline(memory=None,\n",
       "                                                                            steps=[('numerical_imputer',\n",
       "                                                                                    Simple...\n",
       "                                                    verbose=False))],\n",
       "                          verbose=False)),\n",
       "                ('model',\n",
       "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features=None, max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort='deprecated', random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit data\n",
    "pipeline_modeling.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932960893854749"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on new data\n",
    "y_pred = pipeline_modeling.predict(X_test)\n",
    "\n",
    "# Score on new data. Returns the accuracy score\n",
    "pipeline_modeling.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning\n",
    "\n",
    "The most interesting part!\n",
    "\n",
    "Before diving deep into the hyper-parameter tuning, letâ€™s understand the power of using pipelines.\n",
    "- First advantage is that we can tune any parameter of any method that is in the pipeline. \n",
    "- Second advantage is that we can tune different methods too. For example, in the categorical pipeline, we are using OneHotEncoder(). But, there are different methods like OrdinalEncoder(). We can tune the method along with the parameters.\n",
    "\n",
    "Letâ€™s define the parameter grid. We can also pass a list of parameter dictionaries to optimize. In the parameter grid, to define the parameters we want to tune, we have to use the transformer names that were used while creating the pipeline. To backtrack to the parameter, we should use double underscore (__). The below code gives you much clarity on how to backtrack and define the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    "    {\n",
    "        \"preprocessing__column_transformer__numerical_pipeline__numerical_imputer__strategy\": ['median', 'mean'],\n",
    "        \"preprocessing__column_transformer__numerical_pipeline__feature_scaling\": [StandardScaler(), RobustScaler()],\n",
    "        \"model\": [DecisionTreeClassifier()],\n",
    "        \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"model__max_depth\": [10, 20]\n",
    "    },\n",
    "    {\n",
    "        \"preprocessing__column_transformer__categorical_pipeline__categorical_encoder\": [OneHotEncoder(), OrdinalEncoder()],\n",
    "        \"model\": [RandomForestClassifier()],\n",
    "        \"model__max_depth\": [10, 15, 25],\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__bootstrap\": [True, False]\n",
    "    },\n",
    "    {\n",
    "        \"model\": [XGBClassifier()],\n",
    "        \"model__n_estimators\": [10, 50, 100],\n",
    "        \"model__learning_rate\": [0.01, 0.1, 1],\n",
    "        \"model__max_depth\": [3, 6, 9],\n",
    "        \"model__min_child_weight\": [1, 3]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the parameter_grid to GridSearchCV to initialize and call fit() function to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessing',\n",
       "                                        Pipeline(memory=None,\n",
       "                                                 steps=[('drop_features',\n",
       "                                                         DropFeatures(drop_columns=['name',\n",
       "                                                                                    'ticket',\n",
       "                                                                                    'cabin'])),\n",
       "                                                        ('column_transformer',\n",
       "                                                         ColumnTransformer(n_jobs=None,\n",
       "                                                                           remainder='drop',\n",
       "                                                                           sparse_threshold=0.3,\n",
       "                                                                           transformer_weights=None,\n",
       "                                                                           transformers=[('numerical_pipeline',\n",
       "                                                                                          Pipeline(me...\n",
       "                                                  nthread=None,\n",
       "                                                  objective='binary:logistic',\n",
       "                                                  random_state=0, reg_alpha=0,\n",
       "                                                  reg_lambda=1,\n",
       "                                                  scale_pos_weight=1, seed=None,\n",
       "                                                  silent=None, subsample=1,\n",
       "                                                  verbosity=1)],\n",
       "                          'model__learning_rate': [0.01, 0.1, 1],\n",
       "                          'model__max_depth': [3, 6, 9],\n",
       "                          'model__min_child_weight': [1, 3],\n",
       "                          'model__n_estimators': [10, 50, 100]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize grid search\n",
    "grid_search = GridSearchCV(pipeline_modeling, parameter_grid, cv=5, verbose=0)\n",
    "\n",
    "# Fit data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('drop_features',\n",
       "                                  DropFeatures(drop_columns=['name', 'ticket',\n",
       "                                                             'cabin'])),\n",
       "                                 ('column_transformer',\n",
       "                                  ColumnTransformer(n_jobs=None,\n",
       "                                                    remainder='drop',\n",
       "                                                    sparse_threshold=0.3,\n",
       "                                                    transformer_weights=None,\n",
       "                                                    transformers=[('numerical_pipeline',\n",
       "                                                                   Pipeline(memory=None,\n",
       "                                                                            steps=[('numerical_imputer',\n",
       "                                                                                    Simple...\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=1, missing=None,\n",
       "                               n_estimators=100, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best estimator\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212290502793296"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on new data\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can easily try different transformations and select the best pipeline. \n",
    "\n",
    "<b>TIP</b>: We can build pipelines with different transformations and save them for future purposes. We can add new transformations and functions as we go. This will be very helpful in times of competitions and personal use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "def load_data(PATH):\n",
    "    data = pd.read_csv(PATH)\n",
    "    return data\n",
    "\n",
    "titanic_data = load_data('https://raw.githubusercontent.com/mattdelhey/kaggle-titanic/master/Data/train.csv')\n",
    "\n",
    "# Split data 80:20\n",
    "train_data, test_data = train_test_split(titanic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train_data.drop(columns=[\"survived\"])\n",
    "y_train = train_data[\"survived\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"survived\"])\n",
    "y_test = test_data[\"survived\"]\n",
    "\n",
    "# Create new features\n",
    "class CreateNewFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Create a new feature 'family_count' by adding 'sibsp' and 'parch'\n",
    "        \"\"\"\n",
    "        sibsp = self.indices[0]\n",
    "        parch = self.indices[1]\n",
    "        family_count = X[:, sibsp] + X[:, parch]\n",
    "        X = np.c_[X, family_count]\n",
    "        return X\n",
    "\n",
    "# Pipeline for processing numercial features\n",
    "family_count_indices = [2, 3]\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('numerical_imputer', SimpleImputer(strategy='median')),\n",
    "    ('create_new_features', CreateNewFeatures(family_count_indices)),\n",
    "    ('feature_scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for processing categorical features\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('categorical_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('categorical_encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "drop_columns = [\"name\", \"ticket\", \"cabin\"]\n",
    "numerical_columns = X_train.drop(columns=drop_columns).select_dtypes(exclude = \"object\").columns\n",
    "categorical_columns = X_train.drop(columns=drop_columns).select_dtypes(include = \"object\").columns\n",
    "\n",
    "# Column Transformer\n",
    "column_pipeline = ColumnTransformer([\n",
    "    (\"numerical_pipeline\", numerical_pipeline, numerical_columns),\n",
    "    (\"categorical_pipeline\", categorical_pipeline, categorical_columns)\n",
    "])\n",
    "    \n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Drop features\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_columns):\n",
    "        self.drop_columns = drop_columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Drop features\n",
    "        \"\"\"\n",
    "        X = X.drop(columns=self.drop_columns, axis=1)\n",
    "        return X\n",
    "    \n",
    "drop_columns = [\"name\", \"ticket\", \"cabin\"]\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('drop_features', DropFeatures(drop_columns)),\n",
    "    ('column_transformer', column_pipeline)\n",
    "])\n",
    "\n",
    "# 1. Use pipeline for preprocessing features only\n",
    "\n",
    "# Transform the input data\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Train data using XGBoost\n",
    "model = xgboost.XGBClassifier(max_depth=4)\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Transform the data using full_pipeline\n",
    "X_test_processed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Predict on the processed data\n",
    "y_pred = model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred) #(0.815, 0.762)\n",
    "\n",
    "# 2. Include modeling in the pipeline\n",
    "\n",
    "pipeline_modeling = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Fit data\n",
    "pipeline_modeling.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = pipeline_modeling.predict(X_test)\n",
    "\n",
    "# Score on the new data. Returns the accuracy score\n",
    "pipeline_modeling.score(X_test, y_test) # 0.782\n",
    "\n",
    "# 3. Hyper-parameter tuning\n",
    "\n",
    "# Parameter grid\n",
    "parameter_grid = [\n",
    "    {\n",
    "        \"preprocessing__column_transformer__numerical_pipeline__numerical_imputer__strategy\": ['median', 'mean'],\n",
    "        \"preprocessing__column_transformer__numerical_pipeline__feature_scaling\": [StandardScaler(), RobustScaler()],\n",
    "        \"model\": [DecisionTreeClassifier()],\n",
    "        \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"model__max_depth\": [10, 20]\n",
    "    },\n",
    "    {\n",
    "        \"preprocessing__column_transformer__categorical_pipeline__categorical_encoder\": [OneHotEncoder(), OrdinalEncoder()],\n",
    "        \"model\": [RandomForestClassifier()],\n",
    "        \"model__max_depth\": [10, 15, 25],\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__bootstrap\": [True, False]\n",
    "    },\n",
    "    {\n",
    "        \"model\": [XGBClassifier()],\n",
    "        \"model__n_estimators\": [10, 50, 100],\n",
    "        \"model__learning_rate\": [0.01, 0.1, 1],\n",
    "        \"model__max_depth\": [3, 6, 9],\n",
    "        \"model__min_child_weight\": [1, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize grid search\n",
    "grid_search = GridSearchCV(pipeline_modeling, parameter_grid, cv=5, verbose=0)\n",
    "# Fit data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "grid_search.best_estimator_\n",
    "\n",
    "# Score on new data\n",
    "grid_search.score(X_test, y_test) # 0.821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will share you the github link of this notebook in the description below.\n",
    "- Please like, share and subscribe to my channel and don't forget to hit the bell icon.\n",
    "- Check out my website for more articles [https://www.abhishekmamidi.com/](https://www.abhishekmamidi.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
