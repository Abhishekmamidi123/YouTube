{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01 Basics of Apache Spark and PySpark.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNl8SY7QJ5MNUdIDaHhlumd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X8LKFY26itQD"},"source":["## Welcome to the 1st video of this course \"Getting started with Apache Spark\": \n","# Basics of Apache Spark and PySpark\n","\n","![PySpark](https://drive.google.com/uc?id=1oU2tHXn4Tb4NJ0GQLbFQanLUVWj-3M-G)"]},{"cell_type":"markdown","metadata":{"id":"7gvRTt5rgEP4"},"source":["Check out my Introduction video of this course, if you haven't. Here is the [link](https://www.youtube.com/watch?v=2NrWSL_qh3A&list=PLX-qVd8z5JGeolxBVY4APHUnkbFEHJqu5) or click on the above video.\n","\n","<div><img src=\"https://drive.google.com/uc?id=1VxZTnML7NviUOVB_nx_UBMEyp6GjkLwS\" width=\"400\"/>\n","</div></center>"]},{"cell_type":"markdown","metadata":{"id":"KglZpD6DnTGY"},"source":["## Contents\n","- Introduction to Apache Spark and PySpark\n","- Fetaures of PySpark\n","- Run time architecture of PySpark"]},{"cell_type":"markdown","metadata":{"id":"JIlyFxbsmYjb"},"source":["## Introduction\n","- Apache Spark is the most powerful big data tool, which is a parallel distributed processing framework. The core power of Apache Spark is to handle huge amounts of data.\n","- Apache Spark supports the following languages: Scala, Java, Python and R\n","- PySpark is an interface for Apache Spark in Python. In this video, let's spend time to understand the features and internals of Apache Spark and PySpark.\n","- Understanding the underlying concepts will help you relate to the high level operations better.\n","\n","<br>\n","<center><div><img src=\"https://drive.google.com/uc?id=1arWo0Pxd8cADKmrK4XMCDqzmMOo0jyhF\" width=\"400\"/>\n","</div></center>"]},{"cell_type":"markdown","metadata":{"id":"taIG_pu-o5_X"},"source":["## Features of PySpark\n","\n","- It can distribute data across nodes and parallelize tasks\n","- Immutable: You can create a new dataframe by applying transformations on the existing dataframe\n","- Lazy evaluation: It uses DAG for computation\n","- Cache & Persistence: Data can be cached to memory/disk depending on the situation\n","- Fault Tolerance: It can recover data if it's lost\n","- Supports SQL\n","- Supports several modes of deployment: Standalone, Apache Mesos, Hadoop YARN, Kubernetes, etc. These are all different cluster managers\n","  - As we don't have the cluster setup, we will use \"local\" to run Spark on our laptops/Colab. I will tell you more about this during the creation of Spark session.\n","<center>\n","<div><img src=\"https://drive.google.com/uc?id=189zqaqweDmomARu3WHCli86p3JV4Ckhk\" width=\"300\"/>\n","</div></center>\n","\n","- Read data from PostgreSQL, Cassandra, Amazon S3, HDFS, Blob, etc. It supports many other data sources\n","\n","Reference: https://spark.apache.org/"]},{"cell_type":"markdown","metadata":{"id":"6yvjmjMf41zT"},"source":["## Run time Architecture of Apache Spark\n","- It works on a concept called master and slave, where master is driver and slaves are workers/executors. When you do spark-submit (submit the code to driver), the driver creates a Spark context, which is the heart of spark application. Your code is run on executors and the resources are managed by the cluster manager.\n","- Below is the Spark Application image\n","\n","<center>\n","<div><img title=\"Spark Application\" src=\"https://drive.google.com/uc?id=1f908ipDMGQ03A0UewfdrxA6mmuqdk1Yj\" width=\"600\"/>\n","</div></center>\n","\n","- Let's divide the process into steps:\n","  - When you are working on a cluster, you can use use \"spark-submit\" command to launch the application on the cluster. In spark-submit, you can specify number of resources required (executors, driver memory, executor memory, etc), add credentials to data sources, any other config params.\n","    ```\n","    # Spark submit example for Yarn\n","    spark-submit --master yarn \\\n","                 --deploy-mode cluster \\\n","                 --num-executors 10 \\\n","                 --executor-cores 8 \\\n","                 --executor-memory 8g  \\\n","                 --executors-cores 2 \\\n","                 preprocess_data.py arg1\n","\n","    # Spark submit example for Kubernetes\n","    spark-submit --master k8s://192.121.165.213:443 \\\n","                 --deploy-mode cluster \\\n","                 --num-executors 10 \\\n","                 --executors-cores 2 \\\n","                 preprocess_data.py 12\n","    ```\n","    - Reference: https://sparkbyexamples.com/spark/spark-submit-command/\n","<center>\n","<div><img title=\"Spark Application\" src=\"https://drive.google.com/uc?id=1f908ipDMGQ03A0UewfdrxA6mmuqdk1Yj\" width=\"600\"/>\n","</div></center>\n","\n","  - The driver program is launched and the main method of the code runs in the driver. It creates Spark context.\n","    - Spark context establishes a connection to the spark execution environment\n","    - Spark context acts as a master of Spark application\n","  - Based on the configuration provided in the spark-submit, the driver requests the cluster manager to provide the necessary resources for launching executors. The cluster manager launches the required executors (For example: 10 executors with each 8GB of memory)\n","  - One of the responsibility of the driver program is to transform the user code into DAG and convert them to jobs/stages/tasks and schedule them on executors to perform the tasks.\n","    - When the code is executed, Spark context creates DAG, which Directed Acyclic DAG and it optimizes the DAG for optimal execution to save memory and time.\n","  - Once the tasks are executed, it sends back the output to driver program."]},{"cell_type":"markdown","metadata":{"id":"G6EDUPI0dJwQ"},"source":["### Summary:\n","- We have discussed\n","  - What is PySpark and Apache Spark\n","  - Features of PySpark\n","  - Internals of Apache Spark/PySpark\n","- Understanding the internals of Apache Spark is very important in building scalable applications. \n","- NOTE: Writing code itself is not sufficient, syncing the code and configurations plays a major role when it comes to handling big data. Unless you have a bigger picture, you cannot write optimized code."]},{"cell_type":"markdown","metadata":{"id":"Z3IYF2n0eyFZ"},"source":["We are good from theory perspective. The focus will be mainly on hands-on PySpark in the coming videos."]},{"cell_type":"markdown","metadata":{"id":"Iqbdkztcc-Q6"},"source":["### Thank you :)\n","-  That's the end of the this video. If you like this video, please do like, share and subscribe to my channel.\n","<div>\n","<img src=\"https://drive.google.com/uc?id=1ttB2gJaw0cXuJfj6GBx5VaYf2ArjiRXM\" width=\"200\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"2RNDrzovXsx4"},"source":["### References:\n","- https://spark.apache.org/\n","- https://sparkbyexamples.com/spark/spark-submit-command/\n","- https://data-flair.training/blogs/how-apache-spark-works/"]},{"cell_type":"code","metadata":{"id":"K6iFlrACo7Qy"},"source":[""],"execution_count":null,"outputs":[]}]}