{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06 User defined functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZdCWj0J-h37"
      },
      "source": [
        "## Welcome to this course \"Getting started with Apache Spark\"\n",
        "## Video: User defined functions in PySpark\n",
        "\n",
        "![PySpark](https://drive.google.com/uc?id=1oU2tHXn4Tb4NJ0GQLbFQanLUVWj-3M-G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgd2dUX84JY"
      },
      "source": [
        "## Contents\n",
        "- Introduction\n",
        "- User Defined Functions with an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZOGrY2mf5wK"
      },
      "source": [
        "## Setting up the PySpark environment\n",
        "- Check out this video for more details: https://www.youtube.com/watch?v=r5PbUuLUZiE\n",
        "  - You can check out the link in the description below\n",
        "- You can use the below cell to install all the required libraries and files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7C3o1Vqp5De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb382a69-7a6b-47e8-927b-0d660cbc720c"
      },
      "source": [
        "# Setting up the PySpark environment\n",
        "\n",
        "# Install java 8\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Unzip file\n",
        "!tar -xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Install findspark: Adds Pyspark to sys.path at runtime\n",
        "!pip install -q findspark\n",
        "\n",
        "# Install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Add environmental variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "\n",
        "# findspark will locate spark in the system\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 3s (90.9 kB/s)\n",
            "Reading package lists... Done\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tdA-RWoozaM"
      },
      "source": [
        "### Initialize SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VecXeDgQRfgw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Hands-on PySpark on Google Colab\") \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyNSC9w52GXz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "60bcfe74-2728-4880-f89a-fd7ec083b451"
      },
      "source": [
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://510183cd031c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Hands-on PySpark on Google Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f86a4c05ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkxPdO75nLrH"
      },
      "source": [
        "### Read data\n",
        "Dataset (In-vehicle coupon recommendation): https://archive.ics.uci.edu/ml/machine-learning-databases/00603/in-vehicle-coupon-recommendation.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLhz2C8EiP5o"
      },
      "source": [
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00603/in-vehicle-coupon-recommendation.csv -P sample_data/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co-iMsc8V_H-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391b0a6c-493e-4b2a-e1cb-9c13675ebaa8"
      },
      "source": [
        "# We can set header='true' and inferSchema='true' to infer the schema while reading the data\n",
        "\n",
        "filepath = \"sample_data/in-vehicle-coupon-recommendation.csv\"\n",
        "spark_df = spark.read.format('csv').options(header='true', inferSchema='true').load(filepath)\n",
        "spark_df.show(5, truncate=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "|destination    |passanger|weather|temperature|time|coupon               |expiration|gender|age|maritalStatus    |has_children|education               |occupation|income         |car |Bar  |CoffeeHouse|CarryAway|RestaurantLessThan20|Restaurant20To50|toCoupon_GEQ5min|toCoupon_GEQ15min|toCoupon_GEQ25min|direction_same|direction_opp|Y  |\n",
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "|No Urgent Place|Alone    |Sunny  |55         |2PM |Restaurant(<20)      |1d        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |0                |0                |0             |1            |1  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |10AM|Coffee House         |2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |0                |0                |0             |1            |0  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |10AM|Carry out & Take away|2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |1  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |2PM |Coffee House         |2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |0  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |2PM |Coffee House         |1d        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |0  |\n",
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67dhT84UQhli"
      },
      "source": [
        "## Introduction to UDF: User Defined Functions\n",
        "- It extends the capability of Spark by defining our own functions in Python.\n",
        "- PySpark doesn't provide all the possible transformations. For example: you want to select middle letter in a string column. You cannot do it straight away using PySpark built-in functions. In this case, you can write a udf python function and use it to transform a PySpark DataFrame.\n",
        "- If the UDF is not properly created, the you might face performance issues. So, you have to create the function in a optimal way.\n",
        "- NOTE: My suggestion is to use udf functions, if there are no in-built Spark SQL functions to achieve that task.\n",
        "\n",
        "## Steps for using UDF:\n",
        "- Create a python function\n",
        "- Convert the above python function to UDF\n",
        "- Use this UDF function on a PySpark column for transforming data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYl9OrelQqlR"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDhB1fmOQKYV"
      },
      "source": [
        "## Usage of UDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NuaK3xx9KSv",
        "outputId": "e7f25c22-0171-4e7e-8968-1698a4b80ef6"
      },
      "source": [
        "columns_to_use = [\"destination\", \"passanger\", \"weather\", \"time\", \"coupon\", \"income\"]\n",
        "spark_df = spark_df.select(*columns_to_use)\n",
        "spark_df.show(5, truncate=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "|destination    |passanger|weather|time|coupon               |income         |\n",
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "|No Urgent Place|Alone    |Sunny  |2PM |Restaurant(<20)      |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Coffee House         |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Carry out & Take away|$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$37500 - $49999|\n",
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eae5Pl7zGkUW"
      },
      "source": [
        "### Goal: Convert income column to numerical column by taking the average of income range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN5Ya-r4GSV9",
        "outputId": "7b3d7703-f869-4a12-fe0e-6d2d4353a462"
      },
      "source": [
        "spark_df.select(\"income\").distinct().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|          income|\n",
            "+----------------+\n",
            "| $75000 - $87499|\n",
            "| $12500 - $24999|\n",
            "|Less than $12500|\n",
            "| $50000 - $62499|\n",
            "| $25000 - $37499|\n",
            "| $37500 - $49999|\n",
            "| $62500 - $74999|\n",
            "| $87500 - $99999|\n",
            "| $100000 or More|\n",
            "+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mtp-_r0GSSn"
      },
      "source": [
        "## STEP 1: Create a python function\n",
        "\n",
        "def transform_income(income_str):\n",
        "\n",
        "    income_str = str(income_str)\n",
        "\n",
        "    if income_str[0] == \"L\":\n",
        "        income_str = income_str.split(\" \")[-1]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    elif income_str[-1] == \"e\":\n",
        "        income_str = income_str.split(\" \")[0]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    else:\n",
        "        income_str = income_str.split(\" - \")\n",
        "        avg_income = (int(income_str[0][1:]) + int(income_str[1][1:]))/2\n",
        "        return avg_income"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJCbhc2gHS6I",
        "outputId": "efe530d7-7741-4928-8796-7f28c2026eb8"
      },
      "source": [
        "transform_income(\"Less than $12500\"), transform_income(\"$100000 or More\"), transform_income(\"$37500 - $49999\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500.0, 100000.0, 43749.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a9qVWrkJDqp"
      },
      "source": [
        "## STEP 2: Convert python function to UDF function\n",
        "\n",
        "transform_income_udf = F.udf(f=lambda row: transform_income(row), returnType=T.FloatType())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYOzdf9CJlmh",
        "outputId": "9cf02bb7-9f7e-4d33-bfe4-f10527a58859"
      },
      "source": [
        "## STEP 3: Apply the udf function\n",
        "\n",
        "updated_spark_df = spark_df.withColumn(\"income_float\", transform_income_udf(F.col(\"income\")))\n",
        "updated_spark_df.sample(0.2).show(10, truncate=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "|destination    |passanger|weather|time|coupon               |income         |income_float|\n",
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "|No Urgent Place|Alone    |Sunny  |2PM |Restaurant(<20)      |$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Coffee House         |$37500 - $49999|43749.5     |\n",
            "|Home           |Alone    |Sunny  |6PM |Restaurant(20-50)    |$37500 - $49999|43749.5     |\n",
            "|Work           |Alone    |Sunny  |7AM |Coffee House         |$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Alone    |Sunny  |2PM |Restaurant(<20)      |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Bar                  |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Carry out & Take away|$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Restaurant(<20)      |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |6PM |Coffee House         |$62500 - $74999|68749.5     |\n",
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg8A7HnpOhIs",
        "outputId": "b5020a9a-057d-432d-fafc-c862a161ffb3"
      },
      "source": [
        "updated_spark_df.filter(F.col(\"income\") == \"Less than $12500\").show(3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "|    destination|passanger|weather|time|         coupon|          income|income_float|\n",
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "|No Urgent Place|    Alone|  Sunny| 2PM|Restaurant(<20)|Less than $12500|     12500.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|   Coffee House|Less than $12500|     12500.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|            Bar|Less than $12500|     12500.0|\n",
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azz5GbVTOske",
        "outputId": "18f9d4d5-2578-4e3c-9542-40f17b8c6c8b"
      },
      "source": [
        "updated_spark_df.filter(F.col(\"income\") == \"$100000 or More\").show(3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "|    destination|passanger|weather|time|         coupon|         income|income_float|\n",
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "|No Urgent Place|    Alone|  Sunny| 2PM|Restaurant(<20)|$100000 or More|    100000.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|   Coffee House|$100000 or More|    100000.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|            Bar|$100000 or More|    100000.0|\n",
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiMohirvO0Wv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ck1DNB4HBL"
      },
      "source": [
        "## Summary\n",
        "- We have seen how to create and use UDF functions on a PySpark dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgaK8JRu4xnV"
      },
      "source": [
        "### Thank you :)\n",
        "-  That's the end of the this video. If you like this video, please do like, share and subscribe to my channel.\n",
        "- If you are on LinkedIn, please tag me and share your thoughts on this video and the series \"Getting started with PySpark - Hands on\". This will motivate me to make more videos.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1ttB2gJaw0cXuJfj6GBx5VaYf2ArjiRXM\" width=\"200\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCEnrMOm4zO-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}